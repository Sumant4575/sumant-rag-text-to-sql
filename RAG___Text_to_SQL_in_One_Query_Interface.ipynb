{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[1]Let’s build a powerful hybrid system that combines Retrieval-Augmented Generation (RAG) and Text-to-SQL into a single query engine!\n",
        "\n",
        "We’ll leverage @llama_index, @OpenAI, and @arizeAI Phoenix for observability. Let's dive in!"
      ],
      "metadata": {
        "id": "S5irj-JYrAJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Setting Up the Environment\n",
        "\n",
        "First, we install the required packages:"
      ],
      "metadata": {
        "id": "cy3Uy1uWsesG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U llama-index-callbacks-arize-phoenix arize-phoenix\n",
        "!pip install llama-index-llms-openai llama-index-embeddings-openai\n",
        "!pip install llama-index-indices-managed-llama-cloud\n",
        "!pip install llama-index-viz\n"
      ],
      "metadata": {
        "id": "LaKOR4fAsqom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3]Securely Setting Up OpenAI API Key for this"
      ],
      "metadata": {
        "id": "UOUEL5jQyakH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg0J0pAH0FbU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Add here your  openai_api_key\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "iNSWV8qYzHdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4] Integrating Arize Phoenix for RAG and text to sql Model Logging & Observability"
      ],
      "metadata": {
        "id": "BpBIKIiMzKAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup Arize Phoenix for logging/observability\n",
        "import llama_index.core\n",
        "import os\n",
        "\n",
        "PHOENIX_API_KEY = \"Add here your phoenix_api_key\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
        "llama_index.core.set_global_handler(\n",
        "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
        ")"
      ],
      "metadata": {
        "id": "dMKs_5ATzPA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5] Creating and Querying an In-Memory SQL Database with LlamaIndex & OpenAI\"\n",
        "\n",
        "\n",
        "This code snippet sets up an in-memory SQLite database and integrates it with LlamaIndex and OpenAI’s GPT-3.5-turbo for natural language querying. It:\n",
        "\n",
        "Configures LlamaIndex to use OpenAI’s LLM for text-based SQL queries.\n",
        "Creates a SQLite database in memory using SQLAlchemy.\n",
        "Defines a city_stats table with columns for city name, population, and state.\n",
        "Initializes the database schema to store structured data for future AI-driven SQL interactions"
      ],
      "metadata": {
        "id": "AUk6nVeBzz3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SQLDatabase, Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        ")\n",
        "\n",
        "Settings.llm = OpenAI(\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
        "metadata_obj = MetaData()\n",
        "\n",
        "# create city SQL table\n",
        "table_name = \"city_stats\"\n",
        "city_stats_table = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"city_name\", String(16), primary_key=True),\n",
        "    Column(\"population\", Integer),\n",
        "    Column(\"state\", String(16), nullable=False),\n",
        ")\n",
        "\n",
        "metadata_obj.create_all(engine)"
      ],
      "metadata": {
        "id": "7Unj-0KB0Oo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[6] Populating and Querying an In-Memory SQL Database with SQLAlchemy\"\n",
        "\n",
        "\n",
        "This code snippet inserts city population data into an in-memory SQLite database using SQLAlchemy and then queries the data to retrieve all records.\n",
        "\n",
        "Defines a list of cities with their population and state information.\n",
        "Uses SQLAlchemy’s insert() function to add rows to the city_stats table.\n",
        "Executes an SQL query (SELECT * FROM city_stats) to retrieve and print all stored records.\n",
        "Utilizes database transactions (engine.begin()) for efficient and safe data insertion"
      ],
      "metadata": {
        "id": "Y-KVC5np0S45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import insert\n",
        "\n",
        "rows = [\n",
        "    {\"city_name\": \"New York City\", \"population\": 8336000, \"state\": \"New York\"},\n",
        "    {\"city_name\": \"Los Angeles\", \"population\": 3822000, \"state\": \"California\"},\n",
        "    {\"city_name\": \"Chicago\", \"population\": 2665000, \"state\": \"Illinois\"},\n",
        "    {\"city_name\": \"Houston\", \"population\": 2303000, \"state\": \"Texas\"},\n",
        "    {\"city_name\": \"Miami\", \"population\": 449514, \"state\": \"Florida\"},\n",
        "    {\"city_name\": \"Seattle\", \"population\": 749256, \"state\": \"Washington\"},\n",
        "]\n",
        "for row in rows:\n",
        "    stmt = insert(city_stats_table).values(**row)\n",
        "    with engine.begin() as connection:\n",
        "        cursor = connection.execute(stmt)\n",
        "\n",
        "with engine.connect() as connection:\n",
        "    cursor = connection.exec_driver_sql(\"SELECT * FROM city_stats\")\n",
        "    print(cursor.fetchall())"
      ],
      "metadata": {
        "id": "hgixjwwD0hTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[7]AI-Powered Natural Language to SQL Querying with LlamaIndex & OpenAI Embeddings\"\n",
        "\n",
        "\n",
        "This code sets up an AI-driven Text-to-SQL query engine using LlamaIndex and OpenAI Embeddings, enabling users to query structured SQL data using natural language.\n",
        "\n",
        "Initializes an SQLDatabase with an existing SQLite engine and includes the city_stats table.\n",
        "Creates an NLSQLTableQueryEngine that converts natural language queries into SQL queries.\n",
        "Utilizes OpenAI Embeddings to improve query understanding and accuracy"
      ],
      "metadata": {
        "id": "u-vocnZI0sXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding # Import OpenAIEmbedding\n",
        "\n",
        "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
        "sql_query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=sql_database,\n",
        "    tables=[\"city_stats\"],\n",
        "    embed_model=OpenAIEmbedding()\n",
        ")"
      ],
      "metadata": {
        "id": "UDmzKOI4015f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[8] Deploying RAG-Enabled Querying with LlamaCloud Index\n",
        "This code integrates LlamaIndex’s LlamaCloud to enable Retrieval-Augmented Generation (RAG) over structured and unstructured data, allowing for scalable AI-driven querying.\n",
        "\n",
        "Creates a LlamaCloudIndex named \"rag_sql\" within the \"llamacloud_demo\" project.\n",
        "Connects to an organization using API credentials for cloud-based indexing and query execution.\n",
        "Initializes llama_cloud_query_engine to allow AI-powered natural language querying over indexed data"
      ],
      "metadata": {
        "id": "cvJztlEJ1B7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
        "\n",
        "index = LlamaCloudIndex(\n",
        "    name=\"rag_sql\",\n",
        "    project_name=\"llamacloud_demo\",\n",
        "    organization_id=\"07cff857-92f7-49cb-8716-46b683b8d997\",\n",
        "    api_key=\"Add here your Llama-index_api_key\"\n",
        ")\n",
        "\n",
        "llama_cloud_query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "TRYPmKUH1HNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[9] Configuring AI-Powered Query Tools for SQL and RAG-Based Retrieval"
      ],
      "metadata": {
        "id": "ZxXYFH2Q1hCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=sql_query_engine,\n",
        "    description=(\n",
        "        \"Useful for translating a natural language query into a SQL query over\"\n",
        "        \" a table containing: city_stats, containing the population/state of\"\n",
        "        \" each city located in the USA.\"\n",
        "    ),\n",
        "    name=\"sql_tool\"\n",
        ")\n",
        "\n",
        "cities = [\"New York City\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Miami\", \"Seattle\"]\n",
        "llama_cloud_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=llama_cloud_query_engine,\n",
        "    description=(\n",
        "        f\"Useful for answering semantic questions about certain cities in the US.\"\n",
        "    ),\n",
        "    name=\"llama_cloud_tool\"\n",
        ")"
      ],
      "metadata": {
        "id": "2J6ODCvq1q1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[10] Building a Multi-Tool AI Agent for Intelligent Query Routing with LlamaIndex\"\n",
        "\n",
        "\n",
        "This code defines a custom AI agent workflow using LlamaIndex that dynamically routes user queries to the appropriate tool, enabling intelligent, multi-modal interactions.\n",
        "\n",
        "Defines Custom Events & Workflows:\n",
        "\n",
        "InputEvent: Handles user messages.\n",
        "GatherToolsEvent: Identifies relevant tools for the query.\n",
        "ToolCallEvent: Calls the selected tool.\n",
        "ToolCallEventResult: Returns the tool’s response.\n",
        "Implements RouterOutputAgentWorkflow:\n",
        "\n",
        "Stores a chat history to maintain conversational context.\n",
        "Uses OpenAI’s GPT-3.5-Turbo to decide which tool to use.\n",
        "Routes natural language queries to either a SQL-based structured data retriever or an LLM-powered unstructured data retriever (RAG).\n",
        "Supports parallel tool calls for efficiency.\n",
        "Processes Queries in Steps:\n",
        "\n",
        "Prepares chat history.\n",
        "Decides if a tool is needed or responds directly.\n",
        "Dispatches tool calls if necessary.\n",
        "Gathers and returns results to the user"
      ],
      "metadata": {
        "id": "QBhs7qtL12AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "from llama_index.core.tools import BaseTool\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.llms.llm import ToolSelection, LLM\n",
        "from llama_index.core.workflow import (\n",
        "    Workflow,\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    step,\n",
        "     Context,\n",
        ")\n",
        "from llama_index.core.base.response.schema import Response\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "class InputEvent(Event):\n",
        "    \"\"\"Input event.\"\"\"\n",
        "\n",
        "class GatherToolsEvent(Event):\n",
        "    \"\"\"Gather Tools Event\"\"\"\n",
        "\n",
        "    tool_calls: Any\n",
        "\n",
        "class ToolCallEvent(Event):\n",
        "    \"\"\"Tool Call event\"\"\"\n",
        "\n",
        "    tool_call: ToolSelection\n",
        "\n",
        "class ToolCallEventResult(Event):\n",
        "    \"\"\"Tool call event result.\"\"\"\n",
        "\n",
        "    msg: ChatMessage\n",
        "\n",
        "class RouterOutputAgentWorkflow(Workflow):\n",
        "    \"\"\"Custom router output agent workflow.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        tools: List[BaseTool],\n",
        "        timeout: Optional[float] = 10.0,\n",
        "        disable_validation: bool = False,\n",
        "        verbose: bool = False,\n",
        "        llm: Optional[LLM] = None,\n",
        "        chat_history: Optional[List[ChatMessage]] = None,\n",
        "    ):\n",
        "        \"\"\"Constructor.\"\"\"\n",
        "\n",
        "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
        "\n",
        "        self.tools: List[BaseTool] = tools\n",
        "        self.tools_dict: Optional[Dict[str, BaseTool]] = {tool.metadata.name: tool for tool in self.tools}\n",
        "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "        self.chat_history: List[ChatMessage] = chat_history or []\n",
        "\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Resets Chat History\"\"\"\n",
        "\n",
        "        self.chat_history = []\n",
        "\n",
        "    @step()\n",
        "    async def prepare_chat(self, ev: StartEvent) -> InputEvent:\n",
        "        message = ev.get(\"message\")\n",
        "        if message is None:\n",
        "            raise ValueError(\"'message' field is required.\")\n",
        "\n",
        "        # add msg to chat history\n",
        "        chat_history = self.chat_history\n",
        "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
        "        return InputEvent()\n",
        "\n",
        "    @step()\n",
        "    async def chat(self, ev: InputEvent) -> GatherToolsEvent | StopEvent:\n",
        "        \"\"\"Appends msg to chat history, then gets tool calls.\"\"\"\n",
        "\n",
        "        # Put msg into LLM with tools included\n",
        "        chat_res = await self.llm.achat_with_tools(\n",
        "            self.tools,\n",
        "            chat_history=self.chat_history,\n",
        "            verbose=self._verbose,\n",
        "            allow_parallel_tool_calls=True\n",
        "        )\n",
        "        tool_calls = self.llm.get_tool_calls_from_response(chat_res, error_on_no_tool_call=False)\n",
        "\n",
        "        ai_message = chat_res.message\n",
        "        self.chat_history.append(ai_message)\n",
        "        if self._verbose:\n",
        "            print(f\"Chat message: {ai_message.content}\")\n",
        "\n",
        "        # no tool calls, return chat message.\n",
        "        if not tool_calls:\n",
        "            return StopEvent(result=ai_message.content)\n",
        "\n",
        "        return GatherToolsEvent(tool_calls=tool_calls)\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def dispatch_calls(self, ctx: Context, ev: GatherToolsEvent) -> ToolCallEvent:\n",
        "        \"\"\"Dispatches calls.\"\"\"\n",
        "\n",
        "        tool_calls = ev.tool_calls\n",
        "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
        "\n",
        "        # trigger tool call events\n",
        "        for tool_call in tool_calls:\n",
        "            ctx.send_event(ToolCallEvent(tool_call=tool_call))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @step()\n",
        "    async def call_tool(self, ev: ToolCallEvent) -> ToolCallEventResult:\n",
        "        \"\"\"Calls tool.\"\"\"\n",
        "\n",
        "        tool_call = ev.tool_call\n",
        "\n",
        "        # get tool ID and function call\n",
        "        id_ = tool_call.tool_id\n",
        "\n",
        "        if self._verbose:\n",
        "            print(f\"Calling function {tool_call.tool_name} with msg {tool_call.tool_kwargs}\")\n",
        "\n",
        "        # call function and put result into a chat message\n",
        "        tool = self.tools_dict[tool_call.tool_name]\n",
        "        output = await tool.acall(**tool_call.tool_kwargs)\n",
        "        msg = ChatMessage(\n",
        "            name=tool_call.tool_name,\n",
        "            content=str(output),\n",
        "            role=\"tool\",\n",
        "            additional_kwargs={\n",
        "                \"tool_call_id\": id_,\n",
        "                \"name\": tool_call.tool_name\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return ToolCallEventResult(msg=msg)\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def gather(self, ctx: Context, ev: ToolCallEventResult) -> StopEvent | None:\n",
        "        \"\"\"Gathers tool calls.\"\"\"\n",
        "        # wait for all tool call events to finish.\n",
        "        tool_events = ctx.collect_events(ev, [ToolCallEventResult] * await ctx.get(\"num_tool_calls\"))\n",
        "        if not tool_events:\n",
        "            return None\n",
        "\n",
        "        for tool_event in tool_events:\n",
        "            # append tool call chat messages to history\n",
        "            self.chat_history.append(tool_event.msg)\n",
        "\n",
        "        # # after all tool calls finish, pass input event back, restart agent loop\n",
        "        return InputEvent()"
      ],
      "metadata": {
        "id": "WQEbdHp22D9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[11]Initializing an AI-Powered Query Routing Workflow with LlamaIndex"
      ],
      "metadata": {
        "id": "K-d_-sLd2QSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wf = RouterOutputAgentWorkflow(tools=[sql_tool, llama_cloud_tool], verbose=True, timeout=120)"
      ],
      "metadata": {
        "id": "7nFXLFGV2WBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[12]Checking the Installed Version of LlamaIndex"
      ],
      "metadata": {
        "id": "sFOAeb2Z2fHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "print(version(\"llama-index\"))"
      ],
      "metadata": {
        "id": "s3DAXW2X2mn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[13]Visualizing AI Workflow Execution with NetworkX\"\n",
        "\n",
        "\n",
        "This code creates and visualizes a directed graph representing the execution flow of the RouterOutputAgentWorkflow using NetworkX and Matplotlib.\n",
        "\n",
        "Defines a directed graph (DiGraph) to illustrate the step-by-step process.\n",
        "Adds edges to represent workflow transitions, from \"Start\" to \"End\" via multiple steps.\n",
        "Uses nx.draw() to render the graph, with labeled nodes, custom colors, and an organized layout"
      ],
      "metadata": {
        "id": "TCDF1g5G2wja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example graph creation\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from([\n",
        "    (\"Start\", \"RouterOutputAgentWorkflow\"),\n",
        "    (\"RouterOutputAgentWorkflow\", \"Step 1\"),\n",
        "    (\"Step 1\", \"Step 2\"),\n",
        "    (\"Step 2\", \"End\"),\n",
        "])\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(8, 5))\n",
        "nx.draw(G, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", node_size=3000, font_size=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jpuScN3F214Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[14]Executing AI-Powered Natural Language Queries and Displaying Results in Markdown\n",
        "\n",
        "This code runs a natural language query through the AI-powered workflow (wf) and formats the output in Markdown for better readability in Jupyter notebooks.\n",
        "\n",
        "Executes the query using await wf.run(message=\"Which city has the highest population?\"), which processes the request using the LlamaIndex-based agent workflow.\n",
        "Displays the result in Markdown format using display(Markdown(result)), improving the output presentation in Jupyter notebooks"
      ],
      "metadata": {
        "id": "8DyLjUGm3CQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "result = await wf.run(message=\"Which city has the highest population?\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "Nw8wpDRN3KYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await wf.run(message=\"What state is Houston located in?\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "jQa-ONYN3c1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await wf.run(message=\"Where is the Space Needle located?\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "DuQgFyOg3fp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await wf.run(message=\"List all of the places to visit in Miami.\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "NnoWLoEH3iMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await wf.run(message=\"How do people in Chicago get around?\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "migke7hS3mYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await wf.run(message=\"What is the historical name of Los Angeles?\")\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "2UkO4O073rXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}